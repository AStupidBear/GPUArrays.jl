<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Abstract GPU Array implementation"> 
    <meta name="author" content="JuliaGPU"> 
    <link rel="shortcut icon" href="./img/favicon.ico">

    <title>Home - GPUArrays.jl</title>

    <link href="./css/bootstrap-custom.min.css" rel="stylesheet">
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/font-hack/2.018/css/hack.min.css">
    <link href='//fonts.googleapis.com/css?family=PT+Sans:400,400italic,700,700italic&subset=latin-ext,latin' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,700italic,400,300,600,700&subset=latin-ext,latin' rel='stylesheet' type='text/css'>
    <link href="./css/base.css" rel="stylesheet">
    <link href="./css/cinder.css" rel="stylesheet">
    <link rel="stylesheet" href="./css/highlight.css">


    <link href="./assets/Documenter.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

    <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.5.18/webfont.js"></script>
    <script>
    WebFont.load({
        google: {
            families: ['Open Sans', 'PT Sans']
        }
    });
    </script>

    
</head>

<body class="homepage" >

    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            

            <!-- Main title -->

            <a class="navbar-brand" href=".">GPUArrays.jl</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">

            <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                            <i class="fa fa-search"></i> Search
                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/JuliaGPU/GPUArrays.jl/edit/master/docs/index.md"><i class="fa fa-github"></i> Edit on GitHub</a>
                    </li>
            </ul>
        </div>
    </div>
</div>

    <div class="container">
        
        <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="first-level active"><a href="#gpuarrays-documentation">GPUArrays Documentation</a></li>
        <li class="first-level "><a href="#the-abstract-gpu-interface">The Abstract GPU interface</a></li>
        <li class="first-level "><a href="#the-abstract-testsuite">The abstract TestSuite</a></li>
    </ul>
</div></div>
        <div class="col-md-9" role="main">

<p><a id='GPUArrays-Documentation-1'></a></p>
<h1 id="gpuarrays-documentation">GPUArrays Documentation</h1>
<p>GPUArrays is an abstract interface for GPU computations. Think of it as the AbstractArray interface in Julia Base but for GPUs. It allows you to write generic julia code for all GPU platforms and implements common algorithms for the GPU. Like Julia Base, this includes BLAS wrapper, FFTs, maps, broadcasts and mapreduces. So when you inherit from GPUArrays and overload the interface correctly, you will get a lot of functionality for free. This will allow to have multiple GPUArray implementation for different purposes, while maximizing the ability to share code. Currently there are two packages implementing the interface namely <a href="https://github.com/JuliaGPU/CLArrays.jl">CLArrays</a> and <a href="https://github.com/JuliaGPU/CuArrays.jl">CuArrays</a>. As the name suggests, the first implements the interface using OpenCL and the latter uses CUDA.</p>
<p><a id='The-Abstract-GPU-interface-1'></a></p>
<h1 id="the-abstract-gpu-interface">The Abstract GPU interface</h1>
<p>Different GPU computation frameworks like CUDA and OpenCL, have different names for accessing the same hardware functionality. E.g. how to launch a GPU Kernel, how to get the thread index and so forth. GPUArrays offers a unified abstract interface for these functions. This makes it possible to write generic code that can be run on all hardware. GPUArrays itself even contains a pure <a href="https://github.com/JuliaGPU/GPUArrays.jl/blob/master/src/jlbackend.jl">Julia implementation</a> of this interface. The julia reference implementation is a great way to debug your GPU code, since it offers more informative errors and debugging information compared to the GPU backends - which mostly silently error or give cryptic errors (so far).</p>
<p>You can use the reference implementation by using the <code>GPUArrays.JLArray</code> type.</p>
<p>The functions that are currently part of the interface:</p>
<p>The low level dim + idx function, with a similar naming scheme as in CUDA:</p>
<pre><code class="Julia"># with * being either of x, y or z
blockidx_*(state), blockdim_*(state), threadidx_*(state), griddim_*(state)
# Known in OpenCL as:
get_group_id,      get_local_size,    get_local_id,       get_num_groups
</code></pre>

<p>Higher level functionality:</p>
<p><a id='GPUArrays.gpu_call' href='#GPUArrays.gpu_call'>#</a>
<strong><code>GPUArrays.gpu_call</code></strong> &mdash; <em>Function</em>.</p>
<pre><code>gpu_call(kernel::Function, A::GPUArray, args::Tuple, configuration = length(A))
</code></pre>

<p>Calls function <code>kernel</code> on the GPU. <code>A</code> must be an GPUArray and will help to dispatch to the correct GPU backend and supplies queues and contexts. Calls the kernel function with <code>kernel(state, args...)</code>, where state is dependant on the backend and can be used for getting an index into <code>A</code> with <code>linear_index(state)</code>. Optionally, a launch configuration can be supplied in the following way:</p>
<pre><code>1) A single integer, indicating how many work items (total number of threads) you want to launch.
    in this case `linear_index(state)` will be a number in the range `1:configuration`
2) Pass a tuple of integer tuples to define blocks and threads per blocks!
</code></pre>

<p><a target='_blank' href='https://github.com/JuliaGPU/GPUArrays.jl/blob/1bb4cabd8ec7672b1cdd31648073d15ac1966341/src/abstract_gpu_interface.jl#L112-L126' class='documenter-source'>source</a><br></p>
<p><a id='GPUArrays.linear_index-Tuple{Any}' href='#GPUArrays.linear_index-Tuple{Any}'>#</a>
<strong><code>GPUArrays.linear_index</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>linear_index(state)
</code></pre>

<p>linear index corresponding to each kernel launch (in OpenCL equal to get_global_id).</p>
<p><a target='_blank' href='https://github.com/JuliaGPU/GPUArrays.jl/blob/1bb4cabd8ec7672b1cdd31648073d15ac1966341/src/abstract_gpu_interface.jl#L26-L31' class='documenter-source'>source</a><br></p>
<p><a id='GPUArrays.global_size-Tuple{Any}' href='#GPUArrays.global_size-Tuple{Any}'>#</a>
<strong><code>GPUArrays.global_size</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>global_size(state)
</code></pre>

<p>Global size == blockdim * griddim == total number of kernel execution</p>
<p><a target='_blank' href='https://github.com/JuliaGPU/GPUArrays.jl/blob/1bb4cabd8ec7672b1cdd31648073d15ac1966341/src/abstract_gpu_interface.jl#L75-L79' class='documenter-source'>source</a><br></p>
<p><a id='GPUArrays.@linearidx-Tuple{Any,Any}' href='#GPUArrays.@linearidx-Tuple{Any,Any}'>#</a>
<strong><code>GPUArrays.@linearidx</code></strong> &mdash; <em>Macro</em>.</p>
<pre><code>linearidx(A, statesym = :state)
</code></pre>

<p>Macro form of <code>linear_index</code>, which calls return when out of bounds. So it can be used like this:</p>
<pre><code>```julia
function kernel(state, A)
    idx = @linear_index A state
    # from here on it's save to index into A with idx
    @inbounds begin
        A[idx] = ...
    end
end
```
</code></pre>

<p><a target='_blank' href='https://github.com/JuliaGPU/GPUArrays.jl/blob/1bb4cabd8ec7672b1cdd31648073d15ac1966341/src/abstract_gpu_interface.jl#L36-L51' class='documenter-source'>source</a><br></p>
<p><a id='GPUArrays.@cartesianidx-Tuple{Any,Any}' href='#GPUArrays.@cartesianidx-Tuple{Any,Any}'>#</a>
<strong><code>GPUArrays.@cartesianidx</code></strong> &mdash; <em>Macro</em>.</p>
<pre><code>cartesianidx(A, statesym = :state)
</code></pre>

<p>Like <a href=".#GPUArrays.@linearidx-Tuple{Any,Any}"><code>@linearidx(A, statesym = :state)</code></a>, but returns an N-dimensional <code>NTuple{ndim(A), Cuint}</code> as index</p>
<p><a target='_blank' href='https://github.com/JuliaGPU/GPUArrays.jl/blob/1bb4cabd8ec7672b1cdd31648073d15ac1966341/src/abstract_gpu_interface.jl#L62-L66' class='documenter-source'>source</a><br></p>
<p><a id='GPUArrays.synchronize_threads-Tuple{Any}' href='#GPUArrays.synchronize_threads-Tuple{Any}'>#</a>
<strong><code>GPUArrays.synchronize_threads</code></strong> &mdash; <em>Method</em>.</p>
<pre><code> synchronize_threads(state)
</code></pre>

<p>in CUDA terms <code>__synchronize</code> in OpenCL terms: <code>barrier(CLK_LOCAL_MEM_FENCE)</code></p>
<p><a target='_blank' href='https://github.com/JuliaGPU/GPUArrays.jl/blob/1bb4cabd8ec7672b1cdd31648073d15ac1966341/src/abstract_gpu_interface.jl#L15-L20' class='documenter-source'>source</a><br></p>
<p><a id='GPUArrays.device-Tuple{AbstractArray}' href='#GPUArrays.device-Tuple{AbstractArray}'>#</a>
<strong><code>GPUArrays.device</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>device(A::AbstractArray)
</code></pre>

<p>Gets the device associated to the Array <code>A</code></p>
<p><a target='_blank' href='https://github.com/JuliaGPU/GPUArrays.jl/blob/1bb4cabd8ec7672b1cdd31648073d15ac1966341/src/abstract_gpu_interface.jl#L85-L89' class='documenter-source'>source</a><br></p>
<p><a id='GPUArrays.synchronize-Tuple{AbstractArray}' href='#GPUArrays.synchronize-Tuple{AbstractArray}'>#</a>
<strong><code>GPUArrays.synchronize</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>synchronize(A::AbstractArray)
</code></pre>

<p>Blocks until all operations are finished on <code>A</code></p>
<p><a target='_blank' href='https://github.com/JuliaGPU/GPUArrays.jl/blob/1bb4cabd8ec7672b1cdd31648073d15ac1966341/src/abstract_gpu_interface.jl#L95-L99' class='documenter-source'>source</a><br></p>
<p><a id='GPUArrays.@LocalMemory-Tuple{Any,Any,Any}' href='#GPUArrays.@LocalMemory-Tuple{Any,Any,Any}'>#</a>
<strong><code>GPUArrays.@LocalMemory</code></strong> &mdash; <em>Macro</em>.</p>
<p>Creates a local static memory shared inside one block.</p>
<p><a target='_blank' href='https://github.com/JuliaGPU/GPUArrays.jl/blob/1bb4cabd8ec7672b1cdd31648073d15ac1966341/src/ondevice.jl#L21-L23' class='documenter-source'>source</a><br></p>
<p><a id='The-abstract-TestSuite-1'></a></p>
<h1 id="the-abstract-testsuite">The abstract TestSuite</h1>
<p>Since all array packages inheriting from GPUArrays need to offer the same functionality and interface, it makes sense to test them in the same way. This is why GPUArrays contains a test suite which can be called with the array type you want to test.</p>
<p>You can run the test suite like this:</p>
<pre><code class="Julia">using GPUArrays, GPUArrays.TestSuite
TestSuite.run_tests(MyGPUArrayType)
</code></pre>

<p>If you don't want to run the whole suite, you can also run parts of it:</p>
<pre><code class="Julia">Typ = JLArray
GPUArrays.allowslow(false) # fail tests when slow indexing path into Array type is used.

TestSuite.run_gpuinterface(Typ) # interface functions like gpu_call, threadidx, etc
TestSuite.run_base(Typ) # basic functionality like launching a kernel on the GPU and Base operations
TestSuite.run_blas(Typ) # tests the blas interface
TestSuite.run_broadcasting(Typ) # tests the broadcasting implementation
TestSuite.run_construction(Typ) # tests all kinds of different ways of constructing the array
TestSuite.run_fft(Typ) # fft tests
TestSuite.run_linalg(Typ) # linalg function tests
TestSuite.run_mapreduce(Typ) # mapreduce sum, etc
TestSuite.run_indexing(Typ) # indexing tests
</code></pre></div>
        
    </div>

    <footer class="col-md-12 text-center">
        <hr>
        <p>
        <small>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a>.</p></small>
    </footer>

    <script src="./js/jquery-1.10.2.min.js"></script>
    <script src="./js/bootstrap-3.0.3.min.js"></script>
    <script src="./js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script>
    var base_url = '.';
    </script>
    <script data-main="./mkdocs/js/search.js" src="./mkdocs/js/require.js"></script>
    <script src="./js/base.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
    <script src="./assets/mathjaxhelper.js"></script>
    <script src="./search/require.js"></script>
    <script src="./search/search.js"></script>

    <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
        <div class="modal-dialog">
            <div class="modal-content">
                <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal">
                        <span aria-hidden="true">&times;</span>
                        <span class="sr-only">Close</span>
                    </button>
                    <h4 class="modal-title" id="exampleModalLabel">Search</h4>
                </div>
                <div class="modal-body">
                    <p>
                        From here you can search these documents. Enter your search terms below.
                    </p>
                    <form role="form">
                        <div class="form-group">
                            <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query">
                        </div>
                    </form>
                    <div id="mkdocs-search-results"></div>
                </div>
                <div class="modal-footer">
                </div>
            </div>
        </div>
    </div>

    </body>

</html>

<!--
MkDocs version : 0.17.3
Build Date UTC : 2018-04-20 19:54:04
-->
