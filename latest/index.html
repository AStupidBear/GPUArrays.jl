<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Abstract GPU Array implementation"> 
    <meta name="author" content="JuliaGPU">  
    <link rel="shortcut icon" href="./img/favicon.ico">

    <title>GPUArrays.jl</title>

    <link href="./css/bootstrap-custom.min.css" rel="stylesheet">
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css" rel="stylesheet">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/font-hack/2.018/css/hack.min.css">
    <link href='//fonts.googleapis.com/css?family=PT+Sans:400,400italic,700,700italic&subset=latin-ext,latin' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,700italic,400,300,600,700&subset=latin-ext,latin' rel='stylesheet' type='text/css'>
    <link href="./css/base.css" rel="stylesheet">
    <link href="./css/cinder.css" rel="stylesheet">
    <link rel="stylesheet" href="./css/highlight.css">


    <link href="./assets/Documenter.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

    <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.5.18/webfont.js"></script>
    <script>
    WebFont.load({
        google: {
            families: ['Open Sans', 'PT Sans']
        }
    });
    </script>

    
</head>

<body class="homepage" >

    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            

            <!-- Main title -->
            <a class="navbar-brand" href=".">GPUArrays.jl</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
            

            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                        <i class="fa fa-search"></i> Search
                    </a>
                </li>
                
                
                    <li>
                        <a href="https://github.com/JuliaGPU/GPUArrays.jl">
                            
                                <i class="fa fa-github"></i>
                            
                            GitHub
                        </a>
                    </li>
                
            </ul>
        </div>
    </div>
</div>

    <div class="container">
        
        <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
    
        <li class="first-level active"><a href="#gpuarrays-documentation">GPUArrays Documentation</a></li>
        
    
        <li class="first-level "><a href="#abstract-gpu-interface">Abstract GPU interface</a></li>
        
    
    </ul>
</div></div>
        <div class="col-md-9" role="main">

<p><a id='GPUArrays-Documentation-1'></a></p>
<h1 id="gpuarrays-documentation">GPUArrays Documentation</h1>
<p><a id='Abstract-GPU-interface-1'></a></p>
<h1 id="abstract-gpu-interface">Abstract GPU interface</h1>
<p>GPUArrays supports different platforms like CUDA and OpenCL, which all have different names for function that offer the same functionality on the hardware. E.g. how to call a function on the GPU, how to get the thread index etc. GPUArrays offers an abstract interface for these functions which are overloaded by the packages like <a href="https://github.com/JuliaGPU/CLArrays.jl">CLArrays</a> and <a href="https://github.com/JuliaGPU/CuArrays.jl">CuArrays</a>. This makes it possible to write generic code that can be run on all hardware. GPUArrays itself even contains a pure Julia implementation of this interface. The julia reference implementation is also a great way to debug your GPU code, since it offers many more errors and debugging information compared to the GPU backends - which mostly silently error or give cryptic errors (so far). You can use the reference implementation by using the <code>GPUArrays.JLArray</code> type.</p>
<p>The functions that are currently part of the interface:</p>
<p>The low level dim + idx function, with a similar naming scheme as in CUDA:</p>
<pre><code class="Julia"># with * being either of x, y or z
blockidx_*(state), blockdim_*(state), threadidx_*(state), griddim_*(state)
# Known in OpenCL as:
get_group_id,      get_local_size,    get_local_id,       get_num_groups
</code></pre>

<p><a id='GPUArrays.gpu_call' href='#GPUArrays.gpu_call'>#</a>
<strong><code>GPUArrays.gpu_call</code></strong> &mdash; <em>Function</em>.</p>
<p>Calls function <code>f</code> on the GPU. <code>A</code> must be an GPUArray and will help to dispatch to the correct GPU backend and supplies queues and contexts. Calls kernel with <code>kernel(state, args...)</code>, where state is dependant on the backend and can be used for e.g getting an index into A with <code>linear_index(state)</code>. Optionally, launch configuration can be supplied in the following way:</p>
<pre><code>1) A single integer, indicating how many work items (total number of threads) you want to launch.
    in this case `linear_index(state)` will be a number in the range 1:configuration
2) Pass a tuple of integer tuples to define blocks and threads per blocks!
</code></pre>

<p><a target='_blank' href='https://github.com/JuliaGPU/GPUArrays.jl/tree/c3815cf4e6339ac04196d8c80860f3e0c418aaea/src/abstract_gpu_interface.jl#L87-L99' class='documenter-source'>source</a><br></p>
<p><a id='GPUArrays.linear_index-Tuple{Any}' href='#GPUArrays.linear_index-Tuple{Any}'>#</a>
<strong><code>GPUArrays.linear_index</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>inear_index(state)
</code></pre>

<p>linear index in a GPU kernel (equal to  OpenCL.get_global_id)</p>
<p><a target='_blank' href='https://github.com/JuliaGPU/GPUArrays.jl/tree/c3815cf4e6339ac04196d8c80860f3e0c418aaea/src/abstract_gpu_interface.jl#L23-L27' class='documenter-source'>source</a><br></p>
<p><a id='GPUArrays.global_size-Tuple{Any}' href='#GPUArrays.global_size-Tuple{Any}'>#</a>
<strong><code>GPUArrays.global_size</code></strong> &mdash; <em>Method</em>.</p>
<p>Global size == blockdim * griddim == total number of kernel execution</p>
<p><a target='_blank' href='https://github.com/JuliaGPU/GPUArrays.jl/tree/c3815cf4e6339ac04196d8c80860f3e0c418aaea/src/abstract_gpu_interface.jl#L56-L58' class='documenter-source'>source</a><br></p>
<p><a id='GPUArrays.@linearidx-Tuple{Any,Any}' href='#GPUArrays.@linearidx-Tuple{Any,Any}'>#</a>
<strong><code>GPUArrays.@linearidx</code></strong> &mdash; <em>Macro</em>.</p>
<p>Macro form of <code>linear_index</code>, which returns when out of bounds</p>
<p><a target='_blank' href='https://github.com/JuliaGPU/GPUArrays.jl/tree/c3815cf4e6339ac04196d8c80860f3e0c418aaea/src/abstract_gpu_interface.jl#L32-L34' class='documenter-source'>source</a><br></p>
<p><a id='GPUArrays.@cartesianidx-Tuple{Any,Any}' href='#GPUArrays.@cartesianidx-Tuple{Any,Any}'>#</a>
<strong><code>GPUArrays.@cartesianidx</code></strong> &mdash; <em>Macro</em>.</p>
<p>Like <code>@linearidx</code>, but returns an N-dimensional <code>NTuple{ndim(A), Cuint}</code> as index</p>
<p><a target='_blank' href='https://github.com/JuliaGPU/GPUArrays.jl/tree/c3815cf4e6339ac04196d8c80860f3e0c418aaea/src/abstract_gpu_interface.jl#L45-L47' class='documenter-source'>source</a><br></p>
<p><a id='GPUArrays.synchronize_threads-Tuple{Any}' href='#GPUArrays.synchronize_threads-Tuple{Any}'>#</a>
<strong><code>GPUArrays.synchronize_threads</code></strong> &mdash; <em>Method</em>.</p>
<p>in CUDA terms <code>__synchronize</code></p>
<p><a target='_blank' href='https://github.com/JuliaGPU/GPUArrays.jl/tree/c3815cf4e6339ac04196d8c80860f3e0c418aaea/src/abstract_gpu_interface.jl#L15-L17' class='documenter-source'>source</a><br></p>
<p><a id='GPUArrays.device-Tuple{AbstractArray}' href='#GPUArrays.device-Tuple{AbstractArray}'>#</a>
<strong><code>GPUArrays.device</code></strong> &mdash; <em>Method</em>.</p>
<p>Gets the device associated to the Array <code>A</code></p>
<p><a target='_blank' href='https://github.com/JuliaGPU/GPUArrays.jl/tree/c3815cf4e6339ac04196d8c80860f3e0c418aaea/src/abstract_gpu_interface.jl#L65-L67' class='documenter-source'>source</a><br></p>
<p><a id='GPUArrays.synchronize-Tuple{AbstractArray}' href='#GPUArrays.synchronize-Tuple{AbstractArray}'>#</a>
<strong><code>GPUArrays.synchronize</code></strong> &mdash; <em>Method</em>.</p>
<p>Blocks until all operations are finished on <code>A</code></p>
<p><a target='_blank' href='https://github.com/JuliaGPU/GPUArrays.jl/tree/c3815cf4e6339ac04196d8c80860f3e0c418aaea/src/abstract_gpu_interface.jl#L72-L74' class='documenter-source'>source</a><br></p></div>
        
    </div>

    <footer class="col-md-12 text-center">
        <hr>
        <p>
        <small>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a>.</p></small>
    </footer>

    <script src="./js/jquery-1.10.2.min.js"></script>
    <script src="./js/bootstrap-3.0.3.min.js"></script>
    <script src="./js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script>
    var base_url = '.';
    </script>
    <script data-main="./mkdocs/js/search.js" src="./mkdocs/js/require.js"></script>
    <script src="./js/base.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
    <script src="./assets/mathjaxhelper.js"></script>

    <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
        <div class="modal-dialog">
            <div class="modal-content">
                <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal">
                        <span aria-hidden="true">&times;</span>
                        <span class="sr-only">Close</span>
                    </button>
                    <h4 class="modal-title" id="exampleModalLabel">Search</h4>
                </div>
                <div class="modal-body">
                    <p>
                        From here you can search these documents. Enter your search terms below.
                    </p>
                    <form role="form">
                        <div class="form-group">
                            <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query">
                        </div>
                    </form>
                    <div id="mkdocs-search-results"></div>
                </div>
                <div class="modal-footer">
                </div>
            </div>
        </div>
    </div>

    </body>

</html>

<!--
MkDocs version : 0.16.3
Build Date UTC : 2017-09-27 09:18:11
-->
